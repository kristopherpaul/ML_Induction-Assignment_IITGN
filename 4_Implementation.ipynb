{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristopherpaul/ML_InductionAssignment_IITGN/blob/main/4_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g6gNvK_f3w5"
      },
      "source": [
        "# **4 Implementation**\n",
        "\n",
        "5. **Generative Adversarial Network** (*Hard*)\\\n",
        "GANs consist of two parts, a generator and a discriminator, which are trained together. The generator creates\\\n",
        "fake data (trying to mimic the real data distribution), and the discriminator tries to tell the difference between\\\n",
        "real and fake data. The training process involves alternately optimizing the generator and discriminator: the\\\n",
        "discriminator is trained to maximize its ability to classify real vs. fake, and the generator is trained to maximize\\\n",
        "the discriminator’s error on the fake data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mMzNncpEXBAa"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uoNM5WabhPeK"
      },
      "outputs": [],
      "source": [
        "# Generator network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "# Discriminator network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset and setting hyperparameters\n",
        "input_size = 100\n",
        "output_size = 784\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "generator = Generator(input_size, output_size)\n",
        "discriminator = Discriminator(output_size, 1)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "gen_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "disc_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNAh-FUn9Cjw",
        "outputId": "e99ec4ee-e7b9-4641-9923-2276cd1c7257"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 150147567.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 33233386.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 35533349.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 8718777.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        batch_size = images.size(0)\n",
        "        images = images.reshape(batch_size, -1)\n",
        "\n",
        "        real_labels = torch.ones(batch_size, 1)\n",
        "        fake_labels = torch.zeros(batch_size, 1)\n",
        "\n",
        "        disc_optimizer.zero_grad()\n",
        "\n",
        "        real_outputs = discriminator(images)\n",
        "        real_loss = criterion(real_outputs, real_labels)\n",
        "\n",
        "        noise = torch.randn(batch_size, input_size)\n",
        "        fake_images = generator(noise)\n",
        "\n",
        "        fake_outputs = discriminator(fake_images)\n",
        "        fake_loss = criterion(fake_outputs, fake_labels)\n",
        "\n",
        "        disc_loss = real_loss + fake_loss\n",
        "\n",
        "        disc_loss.backward()\n",
        "        disc_optimizer.step()\n",
        "\n",
        "        gen_optimizer.zero_grad()\n",
        "\n",
        "        noise = torch.randn(batch_size, input_size)\n",
        "        fake_images = generator(noise)\n",
        "\n",
        "        outputs = discriminator(fake_images)\n",
        "        gen_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        gen_loss.backward()\n",
        "        gen_optimizer.step()\n",
        "\n",
        "        if (i + 1) % 400 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "                  f\"Discriminator Loss: {disc_loss.item():.4f}, \"\n",
        "                  f\"Generator Loss: {gen_loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NMIEmsW-NhD",
        "outputId": "9814e9d3-d59d-42a1-c86d-3403c787ae74"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Discriminator Loss: 1.7748, Generator Loss: 6.6779\n",
            "Epoch [2/50], Discriminator Loss: 0.3485, Generator Loss: 3.9011\n",
            "Epoch [3/50], Discriminator Loss: 0.6889, Generator Loss: 1.2477\n",
            "Epoch [4/50], Discriminator Loss: 1.8770, Generator Loss: 0.4926\n",
            "Epoch [5/50], Discriminator Loss: 0.8229, Generator Loss: 2.0163\n",
            "Epoch [6/50], Discriminator Loss: 0.7264, Generator Loss: 3.1769\n",
            "Epoch [7/50], Discriminator Loss: 1.5940, Generator Loss: 1.3521\n",
            "Epoch [8/50], Discriminator Loss: 1.2722, Generator Loss: 0.7500\n",
            "Epoch [9/50], Discriminator Loss: 1.9881, Generator Loss: 0.6117\n",
            "Epoch [10/50], Discriminator Loss: 2.2096, Generator Loss: 2.2488\n",
            "Epoch [11/50], Discriminator Loss: 1.6231, Generator Loss: 0.6037\n",
            "Epoch [12/50], Discriminator Loss: 1.5974, Generator Loss: 1.5309\n",
            "Epoch [13/50], Discriminator Loss: 0.8987, Generator Loss: 1.4746\n",
            "Epoch [14/50], Discriminator Loss: 1.0130, Generator Loss: 1.0620\n",
            "Epoch [15/50], Discriminator Loss: 1.5123, Generator Loss: 0.5193\n",
            "Epoch [16/50], Discriminator Loss: 1.9072, Generator Loss: 1.4269\n",
            "Epoch [17/50], Discriminator Loss: 1.7162, Generator Loss: 1.1535\n",
            "Epoch [18/50], Discriminator Loss: 3.2966, Generator Loss: 0.8768\n",
            "Epoch [19/50], Discriminator Loss: 1.6310, Generator Loss: 0.6985\n",
            "Epoch [20/50], Discriminator Loss: 1.3174, Generator Loss: 1.9465\n",
            "Epoch [21/50], Discriminator Loss: 1.4709, Generator Loss: 1.0107\n",
            "Epoch [22/50], Discriminator Loss: 1.8106, Generator Loss: 3.4729\n",
            "Epoch [23/50], Discriminator Loss: 1.5008, Generator Loss: 0.5450\n",
            "Epoch [24/50], Discriminator Loss: 1.2085, Generator Loss: 0.8312\n",
            "Epoch [25/50], Discriminator Loss: 1.4295, Generator Loss: 0.8569\n",
            "Epoch [26/50], Discriminator Loss: 1.6239, Generator Loss: 1.4442\n",
            "Epoch [27/50], Discriminator Loss: 1.5744, Generator Loss: 4.0426\n",
            "Epoch [28/50], Discriminator Loss: 1.1161, Generator Loss: 7.9502\n",
            "Epoch [29/50], Discriminator Loss: 1.1923, Generator Loss: 1.0092\n",
            "Epoch [30/50], Discriminator Loss: 3.2434, Generator Loss: 1.3531\n",
            "Epoch [31/50], Discriminator Loss: 1.2784, Generator Loss: 0.5146\n",
            "Epoch [32/50], Discriminator Loss: 2.0361, Generator Loss: 1.5472\n",
            "Epoch [33/50], Discriminator Loss: 1.6212, Generator Loss: 0.6700\n",
            "Epoch [34/50], Discriminator Loss: 1.4090, Generator Loss: 0.7752\n",
            "Epoch [35/50], Discriminator Loss: 1.1015, Generator Loss: 0.9396\n",
            "Epoch [36/50], Discriminator Loss: 1.5745, Generator Loss: 1.3017\n",
            "Epoch [37/50], Discriminator Loss: 1.2297, Generator Loss: 0.6933\n",
            "Epoch [38/50], Discriminator Loss: 1.5461, Generator Loss: 0.4584\n",
            "Epoch [39/50], Discriminator Loss: 1.2510, Generator Loss: 1.0456\n",
            "Epoch [40/50], Discriminator Loss: 1.2086, Generator Loss: 0.7800\n",
            "Epoch [41/50], Discriminator Loss: 1.3338, Generator Loss: 0.6393\n",
            "Epoch [42/50], Discriminator Loss: 1.0664, Generator Loss: 0.8514\n",
            "Epoch [43/50], Discriminator Loss: 1.6426, Generator Loss: 0.8112\n",
            "Epoch [44/50], Discriminator Loss: 1.2001, Generator Loss: 5.8962\n",
            "Epoch [45/50], Discriminator Loss: 1.2358, Generator Loss: 0.7112\n",
            "Epoch [46/50], Discriminator Loss: 1.2675, Generator Loss: 0.7850\n",
            "Epoch [47/50], Discriminator Loss: 1.5016, Generator Loss: 1.9863\n",
            "Epoch [48/50], Discriminator Loss: 1.5468, Generator Loss: 1.1072\n",
            "Epoch [49/50], Discriminator Loss: 1.2742, Generator Loss: 0.6455\n",
            "Epoch [50/50], Discriminator Loss: 1.7092, Generator Loss: 1.1090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZspH9yb-mJ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNXGnri4s8M1/yD16BXgnpX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}