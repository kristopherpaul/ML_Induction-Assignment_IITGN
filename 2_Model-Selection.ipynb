{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOYr+HlEFDSDOhCjty7MvT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristopherpaul/ML_InductionAssignment_IITGN/blob/main/2_Model-Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 Model Selection**\n",
        "\n",
        "**1. Predicting Bacterial property**\\\n",
        "Tree based model like XGBoost would be the preferred model for this task since they outperform neural networks on tabular data.\\\n",
        "Reference: https://arxiv.org/abs/2207.08815\n",
        "\n",
        "**2. Predicting number of people on the beach**\\\n",
        "This can be framed as a multivariate time-series forecasting problem. A suitable model would be Multitask Gaussian Process.\\\n",
        "Reference: https://www.robots.ox.ac.uk/~davidc/pubs/tbme_mtgp.pdf\n",
        "\n",
        "**3. Text-image search engine**\\\n",
        "Two models will be utilised for this task, specifically a CNN to process the image and an RNN to process the text associated with it. A pre-trained CNN like VGG can be used to extract the image embedding, which is the output of its last fully connected layer. A RNN based architecture like LSTM is fed the captions associated with the images in the MS-COCO 2014 dataset in order to generate the text embedding.\\\n",
        "To train the models, a loss function which encourages similarity between embeddings of matching text-image pairs and dissimilarity between embeddings of non-matching pairs is needed. Contrastive loss is a common choice for this task. The models are then trained to minimize the contrastive loss using gradient-based optimization techniques.\\\n",
        "After training, text and image embeddings can be generated for the entire dataset. For image queries, the text embedding is generated by the text model and its nearest neighbours are retrieved. For text queries, the image embedding is generated by the image model and its nearest neighbours are retrieved. In order to efficiently retrieve the nearest neighbours, Approximate Nearest Neighbour techniques from the FAISS library can be used.\n",
        "\n",
        "**4. Matrix Multiplication**\\\n",
        "It is better to use exisiting libraries like numpy than ML models for this problem as the matrices are small in dimension. The matrix multiplication in numpy is optimized for speed as this library is designed to efficiently perform matrix operations and can make use of multiple CPU cores to accelerate computation.\\\n",
        "To optimize for accuracy, higher precision data types can be used which comes with increased computational time. If a GPU is provided, gpu accelerated libraries like cuBLAS can be used to further optimize matrix multiplication."
      ],
      "metadata": {
        "id": "o9Ag5ApF_TDn"
      }
    }
  ]
}