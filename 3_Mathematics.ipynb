{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristopherpaul/ML_InductionAssignment_IITGN/blob/main/3_Mathematics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 Mathematics**\n",
        "\n",
        "**Gaussian Process**\\\n",
        "Consider a function $F : \\mathcal{X} \\rightarrow \\mathbb{R}$ evaluated at a set of inputs, $X = \\{x_{n} \\, \\epsilon \\, \\mathcal{X}\\}_{n=1}^{N}$. Let $F_{X} = [F(x_{1}),...,F(x_{N})]$ be the set of unknown function values for these inputs. if $F_{X}$ is jointly gaussian for any set of $N \\geq 1$ points, then we call $F : \\mathcal{X} \\rightarrow \\mathbb{R}$ a Gaussian process.\\\n",
        "This process is defined by its mean function $m(x) \\, \\epsilon \\, \\mathbb{R}$ and covariance function $\\mathcal{K}(x,x') \\geq 0$ which is any Mercer kernel.\n",
        "\n",
        "**What is a Mercer kernel?**\\\n",
        "Given a set of $N$ datapoints, let us define the Gram matrix as the following $N Ã— N$ similarity matrix:\\\n",
        "\\begin{gather}\n",
        "K =\n",
        "\\begin{pmatrix}\n",
        "\\mathcal{K}(x_{1},x_{1}) & \\cdots & \\mathcal{K}(x_{1},x_{N})\\\\\n",
        "& \\vdots & \\\\\n",
        "\\mathcal{K}(x_{N},x_{1}) & \\cdots & \\mathcal{K}(x_{N},x_{N})\n",
        "\\end{pmatrix}\n",
        "\\end{gather}\n",
        "We say that $\\mathcal{K}$ is a Mercer kernel if the Gram matrix is positive semi-definite for any set of distinct inputs $\\{x_{i}\\}_{i=1}^{N}$.\n",
        "\n",
        "**Why is $\\mathcal{K}(x,x') \\geq 0$ a necessary condition?**\\\n",
        "The covariance matrix is defined as $C = E[X-\\mu][X-\\mu]^{T}$. Let $v \\, \\epsilon \\, \\mathbb{R}^{N}$ be an arbitrary vector, then\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "v^{T}Cv & = E[v^{T}(X-\\mu)][(X-\\mu)^{T}v] \\\\\n",
        " & = E[(X-\\mu)^{T}v]^{T}[(X-\\mu)^{T}v] \\\\\n",
        " & = E[(X-\\mu)^{T}v]^{2} \\geq 0\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "Therefore, the covariance matrix $C$ is positive semi-definite. Since $K$ is a covariance function, it also has to be positive semi-definite implying that $\\mathcal{K}(x,x') \\geq 0$.\n",
        "\n",
        "**Squared exponential (RBF) kernel**\\\n",
        "It is defined as $\\mathcal{K}(x,x')=exp\\left(-\\frac{\\left \\| x-x' \\right \\|^{2}}{2l^{2}}\\right)$. Here $l$ is the length scale parameter of the kernel, i.e, the distance over which differences start to matter. This is a positive semi-definite kernel function and it is the kernel which is going to be used for this task.\n",
        "\n",
        "**Important Results from Probability theory**\n",
        "\n",
        "- Multivariate Joint Gaussian Distribution\n",
        "\n",
        "$$\n",
        "P_{X, Y} = \\left(\n",
        "    \\begin{bmatrix}\n",
        "    \\mu_X \\\\\n",
        "    \\mu_Y \\\\\n",
        "    \\end{bmatrix},\n",
        "    \\begin{bmatrix}\n",
        "    \\Sigma_{XX} &\\Sigma_{XY} \\\\\n",
        "    \\Sigma_{YX} &\\Sigma_{YY} \\\\\n",
        "    \\end{bmatrix}\n",
        "    \\right)\n",
        "$$\n",
        "\n",
        "- Marginalization\n",
        "\n",
        "$$\n",
        "X \\sim \\mathcal{N}(\\mu_X, \\Sigma_{XX})\n",
        "$$\n",
        "\n",
        "$$\n",
        "Y \\sim \\mathcal{N}(\\mu_Y, \\Sigma_{YY})\n",
        "$$\n",
        "\n",
        "$$\n",
        "p_X(x) = \\int_y{p_{X, Y}(x, y) dy} = \\int_y{p_{X|Y}(x|y)p_Y(y)dy}\n",
        "$$\n",
        "\n",
        "- Conditioning\n",
        "\n",
        "$$\n",
        "X|Y \\sim \\mathcal{N}\\left(\\mu_X + \\Sigma_{XY}\\Sigma^{-1}_{YY}(Y - \\mu_Y), \\space\\space \\Sigma_{XX} - \\Sigma_{XY} \\Sigma_{YY}^{-1}\\Sigma_{YX}\\right) \\\\\n",
        "$$\n",
        "$$\n",
        "Y|X \\sim \\mathcal{N}\\left(\\mu_Y + \\Sigma_{YX}\\Sigma_{XX}^{-1}(X - \\mu_X), \\space\\space \\Sigma_{YY} - \\Sigma_{YX} \\Sigma_{XX}^{-1}\\Sigma_{XY}\\right)\n",
        "$$\n",
        "\n",
        "**Gaussian Process Regression**\n",
        "\n",
        "By definition of the GP, the joint distribution $P(F, f | X, X')$ has the form\n",
        "\n",
        "$$\n",
        "    \\begin{bmatrix}\n",
        "    F \\\\\n",
        "    f \\\\\n",
        "    \\end{bmatrix} \\sim \\mathcal{N}\\left(\n",
        "        \\begin{bmatrix}\n",
        "        0 \\\\\n",
        "        0\n",
        "        \\end{bmatrix},\n",
        "        \\begin{bmatrix}\n",
        "        \\Sigma_{X, X} &\\Sigma_{X, X'} \\\\\n",
        "        \\Sigma_{X', X} &\\Sigma_{X', X'}\n",
        "        \\end{bmatrix}\n",
        "    \\right)\n",
        "$$\n",
        "\n",
        "Using the rules for conditioning Gaussians mentioned above, we get\n",
        "\n",
        "$$\n",
        "P(f | F) = \\mathcal{N}(\\mu', \\Sigma')\n",
        "$$\n",
        "\n",
        "where,\n",
        "\n",
        "$$\n",
        "\\mu' = \\Sigma_{X', X} \\Sigma_{X, X}^{-1}F \\\\\n",
        "\\Sigma' = \\Sigma_{X', X'} - \\Sigma_{X', X} \\Sigma_{X, X}^{-1} \\Sigma_{X, X'}\n",
        "$$"
      ],
      "metadata": {
        "id": "e4t-YIX6syvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cholesky Decomposition**\n",
        "\n",
        "The cholesky decomposition decomposes a positive (semi)definite matrix into a lower triangular matrix, $L$, with real and non-zero diagonal entries and $L^T$, the transpose of $L$.\n",
        "\n",
        "$$\n",
        "A = LL^T\n",
        "$$\n",
        "\n",
        "**Importance of cholesky decomposition**\n",
        "\n",
        "*Efficient Computation*, Cholesky decomposition allows for efficient and numerically stable computation of inversion and manipulation of covariance matrices, which otherwise can be quite computationally demanding, especially for large datasets.\n",
        "\n",
        "**How to find the inverse of a matrix via cholesky decomposition**\n",
        "\n",
        "$$\n",
        "A = LL^{T} \\\\\n",
        "A^{-1} = (LL^{T})^{-1} = L^{-T}L^{-1}\n",
        "$$\n",
        "\n",
        "The above expression is much computationally efficient to compute than a direct inverse."
      ],
      "metadata": {
        "id": "YWcg4fYUWG6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extrapolation on new data**\n",
        "\n",
        "In the context of GPR, \"extrapolation\" refers to making predictions for inputs that are outisde the range of training data. The quality of extrapolation can be assessed by evaluating how well the surrogate function captures the underlying trends and patterns of the true function beyond the observed data points.\n",
        "\n",
        "The kernel function in GP model can have parameters that need to be optimized. The optimization of kernel parameters is an important step in GPR to improve the model's fit to the data.\n",
        "\n",
        "To optimize the parameters of the kernel function, Maximum Likelihood Estimation (MLE) or maximum a posteriori (MAP) estimation. The goal is to find the values of the kernel parameters that maximize the likelihood or posterior probability of the observed data.\n",
        "\n",
        "GPR differs from regular neural networks in terms of optimization. Neural networks typically involve an optimization step where model parameters are iteratively updated to minimize a defined loss function. This optimization process often employs gradient-based methods, such as backpropogation and stochastic gradient descent, to find the optimal parameters that minimize the prediction error on the training data.\n",
        "\n",
        "However, GPR does not involve such kind of parameter optimization, where it only employs bayesian based approaches like MLE and MAP as mentioned above to optimize for the parameters such as length scale and variance of the kernel (RBF).\n",
        "\n",
        "**Extension to classification**\n",
        "\n",
        "Gaussian Process regression can be extended to perform classification by using appropriate likelihood functions. The standard Gaussian Process framework assumes a Gaussian likelihood for regression tasks. To perform classification, a different likelihood function, such as the probit or logistic likelihood, can be used. The choice of the likelihood function would depend on the specific classification problem and the desired behavior of the surrogate function. By adjusting the likelihood function and the associated inference methods, Gaussian Process can be adapted for classification tasks.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f67eQKTMbr-F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5XYEGF2RdoXO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}